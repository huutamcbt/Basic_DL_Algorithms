{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataScaling:\n",
    "    def simple_features_scaling(self, pData):\n",
    "        max = np.max(pData)\n",
    "        \n",
    "        return pData / max  \n",
    "    \n",
    "    def min_max_scaling(self, pData):\n",
    "        min = np.min(pData)\n",
    "        max = np.max(pData)\n",
    "\n",
    "        return (pData - min) / (max - min)\n",
    "    \n",
    "\n",
    "class Normalization:\n",
    "    def standardization(self, pData):\n",
    "        std = np.std(pData)\n",
    "        mean = np.mean(pData)\n",
    "\n",
    "        return (pData - mean) / std\n",
    "    \n",
    "    def mean_normal(self, pData):\n",
    "        mean = np.mean(pData)\n",
    "        max = np.max(pData)\n",
    "        min = np.min(pData)\n",
    "\n",
    "        return (pData - mean) / (max - min)\n",
    "\n",
    "    def box_cox_normal(self, pData):\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (60000, 784)\n",
      "test_x's shape: (10000, 784)\n",
      "[[-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  -0.00000000e+00 -1.84206807e+01 -0.00000000e+00 -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-1.84092220e+01 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  -2.85872480e-03 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-0.00000000e+00 -1.68995744e+01 -0.00000000e+00 -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  -0.00000000e+00 -1.35040367e+01]]\n",
      "Cost:  1.2440543956462282 Train Accuracy: 15.691666666666668\n",
      "[[ -0.          -0.          -0.          -0.          -0.\n",
      "  -18.42067416  -0.          -0.          -0.          -0.        ]\n",
      " [-15.6746506   -0.          -0.          -0.          -0.\n",
      "   -0.          -0.          -0.          -0.          -0.        ]\n",
      " [ -0.          -0.          -0.          -0.          -0.89282598\n",
      "   -0.          -0.          -0.          -0.          -0.        ]\n",
      " [ -0.          -2.96490899  -0.          -0.          -0.\n",
      "   -0.          -0.          -0.          -0.          -0.        ]\n",
      " [ -0.          -0.          -0.          -0.          -0.\n",
      "   -0.          -0.          -0.          -0.          -3.75088265]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 156\u001b[0m\n\u001b[0;32m    153\u001b[0m layers_dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m    155\u001b[0m ann \u001b[38;5;241m=\u001b[39m ArtificialNeuralNetwork(layers_dims)\n\u001b[1;32m--> 156\u001b[0m \u001b[43mann\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ann\u001b[38;5;241m.\u001b[39mpredict(train_x, train_y))\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ann\u001b[38;5;241m.\u001b[39mpredict(test_x, test_y))\n",
      "Cell \u001b[1;32mIn[16], line 101\u001b[0m, in \u001b[0;36mArtificialNeuralNetwork.fit\u001b[1;34m(self, pX, py, learning_rate, n_iterations, mini_batch)\u001b[0m\n\u001b[0;32m     99\u001b[0m cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(y \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(A\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m))\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m((y \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(A\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m))[:\u001b[38;5;241m5\u001b[39m])\n\u001b[1;32m--> 101\u001b[0m derivatives \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)] \u001b[38;5;241m-\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m derivatives[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)]\n",
      "Cell \u001b[1;32mIn[16], line 72\u001b[0m, in \u001b[0;36mArtificialNeuralNetwork.backward\u001b[1;34m(self, pX, py, store)\u001b[0m\n\u001b[0;32m     69\u001b[0m derivatives[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL)] \u001b[38;5;241m=\u001b[39m db\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 72\u001b[0m     dZ \u001b[38;5;241m=\u001b[39m dAPrev \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mZ\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     dW \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m*\u001b[39m dZ\u001b[38;5;241m.\u001b[39mdot(store[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     74\u001b[0m     db \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[16], line 27\u001b[0m, in \u001b[0;36mArtificialNeuralNetwork.sigmoid_derivative\u001b[1;34m(self, pZ)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid_derivative\u001b[39m(\u001b[38;5;28mself\u001b[39m, pZ):\n\u001b[1;32m---> 27\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpZ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m s)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class ArtificialNeuralNetwork:\n",
    "    def __init__(self, layers_size):\n",
    "        # Init the layers size for Neural Network. For example layers_size = [2, 2, 1]\n",
    "        self.layers_size = layers_size\n",
    "        # Store all parameters including weight, bias, derivative\n",
    "        self.parameters = {}\n",
    "        # Initialize the length of layers size\n",
    "        self.L = len(self.layers_size)\n",
    "        # Initialize the length of data row\n",
    "        self.n = 0\n",
    "        # Save all cost for loop of each hidden layer\n",
    "        self.costs = []\n",
    "\n",
    "    # Initialize the value for weights and biases in network\n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(1)\n",
    "\n",
    "        for l in range(1, len(self.layers_size)):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_size[l], self.layers_size[l - 1])\n",
    "            self.parameters[\"b\" + str(l)] = np.random.randn(self.layers_size[l], 1)\n",
    "\n",
    "    # This activation function is used in hidden layer of ANN\n",
    "    def sigmoid(self, pZ):  \n",
    "        return 1 / (1 + np.exp(-pZ))\n",
    "    \n",
    "    def sigmoid_derivative(self, pZ):\n",
    "        s = self.sigmoid(pZ)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    # This activation function is used in final layer of ANN\n",
    "    # pZ is array of z input\n",
    "    def softmax(self, pZ):\n",
    "        expZ = np.exp(pZ - np.max(pZ))\n",
    "        return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "    \n",
    "    def forward(self, pX):\n",
    "        store = {}\n",
    "\n",
    "        A = pX.T\n",
    "        for l in range(self.L - 1):\n",
    "            Z = np.dot(self.parameters[\"W\" + str(l + 1)], A) + self.parameters[\"b\" + str(l + 1)]\n",
    "            A = self.sigmoid(Z)\n",
    "            store[\"A\" + str(l + 1)] = A\n",
    "            store[\"Z\" + str(l + 1)] = Z\n",
    "            store[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)]\n",
    "\n",
    "        Z = np.dot(self.parameters[\"W\" + str(self.L)], A) + self.parameters[\"b\" + str(self.L)]\n",
    "        A = self.softmax(Z)\n",
    "        store[\"A\" + str(self.L)] = A\n",
    "        store[\"Z\" + str(self.L)] = Z\n",
    "        store[\"W\" + str(self.L)] = self.parameters[\"W\" + str(self.L)]\n",
    "\n",
    "        return A, store\n",
    "    \n",
    "    def backward(self, pX, py, store):\n",
    "        # This variable stores all derivative of weights and biases\n",
    "        derivatives = {}\n",
    "\n",
    "        store[\"A0\"] = pX.T\n",
    "\n",
    "        A = store[\"A\" + str(self.L)]\n",
    "        dZ = A - py.T\n",
    "\n",
    "        dW = dZ.dot(store[\"A\" + str(self.L - 1)].T) / self.n\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / self.n\n",
    "        dAPrev = store[\"W\" + str(self.L)].T.dot(dZ)\n",
    "    \n",
    "        derivatives[\"dW\" + str(self.L)] = dW\n",
    "        derivatives[\"db\" + str(self.L)] = db\n",
    "    \n",
    "        for l in range(self.L - 1, 0, -1):\n",
    "            dZ = dAPrev * self.sigmoid_derivative(store[\"Z\" + str(l)])\n",
    "            dW = 1. / self.n * dZ.dot(store[\"A\" + str(l - 1)].T)\n",
    "            db = 1. / self.n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "            if l > 1:\n",
    "                dAPrev = store[\"W\" + str(l)].T.dot(dZ)\n",
    "\n",
    "            derivatives[\"dW\" + str(l)] = dW\n",
    "            derivatives[\"db\" + str(l)] = db\n",
    "    \n",
    "        return derivatives\n",
    "\n",
    "    def fit(self, pX, py, learning_rate=0.01, n_iterations=2500, mini_batch=50):\n",
    "        np.random.seed(1)\n",
    "    \n",
    "        self.n = pX.shape[0]\n",
    "    \n",
    "        self.layers_size.insert(0, pX.shape[1])\n",
    "    \n",
    "        self.initialize_parameters()\n",
    "        for loop in range(n_iterations):\n",
    "            for i in range(0, self.n, mini_batch):\n",
    "                X = pX[i:i + mini_batch]\n",
    "                y = py[i:i + mini_batch]\n",
    "                A, store = self.forward(X)\n",
    "                \n",
    "                cost = -np.mean(y * np.log(A.T + 1e-8))\n",
    "                print((y * np.log(A.T + 1e-8))[:5])\n",
    "                derivatives = self.backward(X, y, store)\n",
    "\n",
    "                for l in range(1, self.L + 1):\n",
    "                    self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * derivatives[\"dW\" + str(l)]\n",
    "                    self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * derivatives[\"db\" + str(l)]\n",
    "\n",
    "                if loop % 100 == 0:\n",
    "                    print(\"Cost: \", cost, \"Train Accuracy:\", self.predict(X, y))\n",
    "\n",
    "                if loop % 10 == 0:\n",
    "                    self.costs.append(cost)\n",
    "\n",
    "    def predict(self, pX, py):\n",
    "        A, cache = self.forward(pX)\n",
    "        y_hat = np.argmax(A, axis=0)\n",
    "        py = np.argmax(py, axis=1)\n",
    "        accuracy = (y_hat == py).mean()\n",
    "        return accuracy * 100\n",
    "\n",
    "    def plot_cost(self):\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()\n",
    "\n",
    "def pre_process_data(train_x, train_y, test_x, test_y):\n",
    "    # Normalize\n",
    "    train_x = train_x / 255.\n",
    "    test_x = test_x / 255.\n",
    "\n",
    "    enc = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "    train_y = enc.fit_transform(train_y.reshape(len(train_y), -1))\n",
    "    \n",
    "    test_y = enc.transform(test_y.reshape(len(test_y), -1))\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    (train_x, train_y ), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "    train_x = train_x.reshape(-1, 784)\n",
    "    test_x = test_x.reshape(-1, 784)\n",
    "    train_x = train_x.astype('float32')\n",
    "    test_x = test_x.astype('float32')\n",
    "\n",
    "    train_x, train_y, test_x, test_y = pre_process_data(train_x, train_y, test_x, test_y)\n",
    "    \n",
    "    print(\"train_x's shape: \" + str(train_x.shape))\n",
    "    print(\"test_x's shape: \" + str(test_x.shape))\n",
    "    \n",
    "    layers_dims = [50, 512, 10]\n",
    "    \n",
    "    ann = ArtificialNeuralNetwork(layers_dims)\n",
    "    ann.fit(train_x, train_y, learning_rate=0.1, n_iterations=1000, mini_batch=60000)\n",
    "    print(\"Train Accuracy:\", ann.predict(train_x, train_y))\n",
    "    print(\"Test Accuracy:\", ann.predict(test_x, test_y))\n",
    "    ann.plot_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = ArtificialNeuralNetwork([2,2,1])\n",
    "\n",
    "a = np.arange(9).reshape(3,3)\n",
    "np.argmax(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
